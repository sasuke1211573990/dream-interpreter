import os
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
from typing import Optional, List, Dict

class DreamInterpreter:
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.loading_steps = []
        self.inference_steps = []
        self.model = None
        self.tokenizer = None
        self.use_llm = False
        
        os.environ.setdefault("HF_ENDPOINT", "https://hf-mirror.com")
        os.environ.setdefault("HF_HUB_BASE_URL", "https://hf-mirror.com")
        for k in ["HTTP_PROXY", "HTTPS_PROXY", "ALL_PROXY", "http_proxy", "https_proxy", "all_proxy"]:
            if k in os.environ:
                os.environ.pop(k)

        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        local_model_path = self._find_local_model()
        if local_model_path:
            self.model_name = local_model_path
            self._log_step(f"ğŸ¯ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")
        else:
            self.model_name = os.environ.get("MODEL_NAME", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
            self._log_step(f"ğŸŒ ä½¿ç”¨è¿œç¨‹æ¨¡å‹: {self.model_name}")
        
        try:
            self._log_step(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹: {self.model_name}")
            self._log_step(f"ğŸ“¡ ä½¿ç”¨é•œåƒ: {os.environ.get('HF_ENDPOINT')}")

            self._log_step("ğŸ“š æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
            start_time = time.time()
            
            # é¦–å…ˆæ£€æŸ¥æ¨¡å‹é…ç½®
            config = AutoConfig.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                local_files_only=local_model_path is not None
            )
            self._log_step(f"ğŸ” æ¨¡å‹ç±»å‹: {config.model_type}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                local_files_only=local_model_path is not None
            )
            load_time = time.time() - start_time
            self._log_step(f"âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}s)")

            if torch.cuda.is_available():
                self._log_step(f"ğŸ® CUDAå¯ç”¨ï¼ŒGPU: {torch.cuda.get_device_name(0)}")
            else:
                self._log_step("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")

            self._log_step("ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹(è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
            start_time = time.time()
            load_kwargs = {
                "trust_remote_code": True,
                "local_files_only": local_model_path is not None,
            }
            if torch.cuda.is_available():
                load_kwargs["torch_dtype"] = torch.float16
                load_kwargs["device_map"] = "auto"
            else:
                load_kwargs["torch_dtype"] = torch.float32

            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹å¼
            if config.model_type == "qwen2":
                from transformers import Qwen2ForCausalLM
                self.model = Qwen2ForCausalLM.from_pretrained(
                    self.model_name,
                    **load_kwargs,
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **load_kwargs,
                )
            if not torch.cuda.is_available():
                self.model = self.model.to("cpu")
            load_time = time.time() - start_time
            self._log_step(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}s)")
            self.use_llm = True
        
            if hasattr(self.model, 'config'):
                self._log_step(f"ğŸ“Š æ¨¡å‹å‚æ•°: {getattr(self.model.config, 'n_parameters', 'æœªçŸ¥')}")
                self._log_step(f"ğŸ“ è¯æ±‡è¡¨å¤§å°: {getattr(self.model.config, 'vocab_size', 'æœªçŸ¥')}")
        except Exception as e:
            self._log_step(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨è§„åˆ™å¼•æ“: {e}")

    def _find_local_model(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ¬åœ°DeepSeekæ¨¡å‹è·¯å¾„"""
        possible_paths = [
            # Windowså¸¸è§è·¯å¾„
            os.path.expanduser("~\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B\\snapshots"),
            "D:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
            "E:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
            "./models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
            "../models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        ]
        
        for base_path in possible_paths:
            if os.path.exists(base_path):
                if "snapshots" in base_path:
                    # å¦‚æœæ˜¯snapshotsç›®å½•ï¼ŒæŸ¥æ‰¾æœ€æ–°ç‰ˆæœ¬
                    try:
                        subdirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
                        if subdirs:
                            latest_snapshot = os.path.join(base_path, sorted(subdirs)[-1])
                            config_path = os.path.join(latest_snapshot, "config.json")
                            if os.path.exists(config_path):
                                return latest_snapshot
                    except Exception:
                        pass
                else:
                    # ç›´æ¥æ£€æŸ¥æ¨¡å‹ç›®å½•
                    config_path = os.path.join(base_path, "config.json")
                    if os.path.exists(config_path):
                        return base_path
        
        return None

    def _log_step(self, message: str):
        if self.verbose:
            print(f"[{time.strftime('%H:%M:%S')}] {message}")
        self.loading_steps.append(message)
    
    def _log_inference(self, message: str):
        if self.verbose:
            print(f"ğŸ¤” {message}")
        self.inference_steps.append(message)
    
    def interpret(self, text: str) -> str:
        self.inference_steps = []  # æ¸…ç©ºä¹‹å‰çš„æ¨ç†æ­¥éª¤
        
        self._log_inference(f"ğŸ“ æ”¶åˆ°æ¢¦å¢ƒæè¿°: {text[:50]}{'...' if len(text) > 50 else ''}")
        if not self.model or not self.tokenizer or not self.use_llm:
            self._log_inference("âš ï¸ æœªåŠ è½½å¤§æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ™å¼•æ“è¿›è¡Œè§£æ")
            return self._fallback_interpret(text)
        
        messages = [
            {"role": "user", "content": f"è¯·å¸®æˆ‘è¯¦ç»†è§£æè¿™ä¸ªæ¢¦å¢ƒï¼Œå¹¶ç»™å‡ºå¿ƒç†å­¦å»ºè®®ï¼š\n{text}"}
        ]
        
        self._log_inference("ğŸ”§ æ­£åœ¨æ„å»ºè¾“å…¥æ¨¡æ¿...")
        text_input = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        self._log_inference(f"ğŸ“„ è¾“å…¥æ¨¡æ¿é•¿åº¦: {len(text_input)} å­—ç¬¦")
        
        # åˆ†è¯
        self._log_inference("ğŸ” æ­£åœ¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯...")
        device = next(self.model.parameters()).device
        model_inputs = self.tokenizer(
            [text_input],
            return_tensors="pt",
            padding=True,
        ).to(device)
        
        input_length = model_inputs.input_ids.shape[1]
        self._log_inference(f"ğŸ“Š è¾“å…¥tokenæ•°é‡: {input_length}")
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        max_new_tokens = int(os.environ.get("MAX_NEW_TOKENS", "256"))
        self._log_inference(f"âš™ï¸ ç”Ÿæˆå‚æ•°è®¾ç½®: max_new_tokens={max_new_tokens}, temperature=0.7, top_p=0.9")
        
        # å¼€å§‹ç”Ÿæˆ
        self._log_inference("ğŸš€ å¼€å§‹ç”Ÿæˆå›å¤...")
        start_time = time.time()
        
        # ä½¿ç”¨æ›´è¯¦ç»†çš„ç”Ÿæˆè¿‡ç¨‹
        generated_ids = self.model.generate(
            input_ids=model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
            # æ·»åŠ å›è°ƒå‡½æ•°æ¥æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
            output_scores=True,
            return_dict_in_generate=True,
        )
        
        generation_time = time.time() - start_time
        
        # å¤„ç†ç”Ÿæˆçš„ID
        generated_ids_output = generated_ids.sequences
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids_output)
        ]
        
        output_length = len(generated_ids[0])
        self._log_inference(f"âœ… ç”Ÿæˆå®Œæˆ!")
        self._log_inference(f"ğŸ“ è¾“å‡ºtokenæ•°é‡: {output_length}")
        self._log_inference(f"â±ï¸ ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
        self._log_inference(f"âš¡ ç”Ÿæˆé€Ÿåº¦: {output_length/generation_time:.2f} tokens/ç§’")

        # è§£ç 
        self._log_inference("ğŸ”¤ æ­£åœ¨è§£ç è¾“å‡º...")
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        self._log_inference(f"ğŸ“¤ æœ€ç»ˆå›å¤é•¿åº¦: {len(response)} å­—ç¬¦")
        
        return response

    def _fallback_interpret(self, text: str) -> str:
        self._log_inference("å¼€å§‹åˆ†ææ¢¦å¢ƒå…³é”®è¯")
        lower = text.lower()
        analysis_parts = []
        if "è›‡" in text or "snake" in lower:
            analysis_parts.append("æ¢¦åˆ°è›‡å¾€å¾€å’Œæ½œæ„è¯†ä¸­çš„ç´§å¼ ã€å‹åŠ›æˆ–è€…å¯¹æœªçŸ¥çš„æ‹…å¿§æœ‰å…³ã€‚")
        if "è¿½" in text or "è¿½èµ¶" in text or "chase" in lower:
            analysis_parts.append("è¢«è¿½èµ¶é€šå¸¸ä»£è¡¨ä½ åœ¨ç°å®ä¸­æœ‰æƒ³é€ƒé¿çš„é—®é¢˜æˆ–å‹åŠ›ã€‚")
        if "æ‰" in text or "å è½" in text or "fall" in lower:
            analysis_parts.append("ä¸‹å æ„Ÿç»å¸¸å’Œä¸å®‰å…¨æ„Ÿã€å¯¹æœªæ¥çš„ä¸ç¡®å®šæœ‰å…³ã€‚")
        if "è€ƒè¯•" in text or "test" in lower or "è€ƒè¯•" in text:
            analysis_parts.append("è€ƒè¯•åœºæ™¯å¤šå’Œè‡ªæˆ‘è¦æ±‚ã€ç„¦è™‘ä»¥åŠå¯¹è¯„ä»·çš„æ‹…å¿ƒç›¸å…³ã€‚")
        if not analysis_parts:
            analysis_parts.append("è¿™ä¸ªæ¢¦å¢ƒåæ˜ å‡ºä½ æœ€è¿‘æƒ…ç»ªä¸Šå­˜åœ¨ä¸€å®šçš„ç´§å¼ å’Œä¸å®‰ã€‚")
        self._log_inference("ç»¼åˆæƒ…èŠ‚å’Œæƒ…ç»ªè¿›è¡Œæ•´ä½“åˆ¤æ–­")
        interpretation = "æ¢¦å¢ƒè§£æï¼š\n" + "\n".join(f"- {p}" for p in analysis_parts)
        suggestion = "\n\nå¿ƒç†å»ºè®®ï¼š\n- å»ºè®®ä½ ç•™æ„æœ€è¿‘è®©ä½ æ„Ÿåˆ°ç´§å¼ æˆ–è¢«è¿½èµ¶çš„äº‹æƒ…ã€‚\n- å¯ä»¥é€šè¿‡è®°å½•æƒ…ç»ªã€å’Œä¿¡ä»»çš„äººæ²Ÿé€šæ¥é‡Šæ”¾å‹åŠ›ã€‚\n- ä¿æŒè§„å¾‹ä½œæ¯å’Œé€‚åº¦æ”¾æ¾ï¼Œæœ‰åŠ©äºå‡è½»è¿™ç±»æ¢¦å¢ƒå¸¦æ¥çš„å½±å“ã€‚"
        return interpretation + suggestion

    def print_inference_summary(self):
        print("\n" + "="*60)
        print("ğŸ§  æ¨ç†è¿‡ç¨‹æ€»ç»“:")
        print("="*60)
        for i, step in enumerate(self.inference_steps, 1):
            print(f"{i:2d}. {step}")
        print("="*60)
        
    def get_model_info(self) -> Dict:
        return {
            "model_name": self.model_name,
            "device": str(next(self.model.parameters()).device) if self.model is not None else "cpu",
            "loading_steps": self.loading_steps,
            "total_loading_steps": len(self.loading_steps)
        }

if __name__ == "__main__":
    print("ğŸŒ™ æ¢¦å¢ƒè§£æAIåŠ©æ‰‹ (DeepSeek-R1)")
    print("="*60)
    
    interpreter = DreamInterpreter(verbose=True)
    
    print("\nâœ¨ æ¨¡å‹åŠ è½½å®Œæˆï¼å¯ä»¥å¼€å§‹è§£ææ¢¦å¢ƒäº†")
    print("ğŸ’¡ æç¤ºï¼šè¾“å…¥æ‚¨çš„æ¢¦å¢ƒæè¿°ï¼Œæˆ‘ä¼šä¸ºæ‚¨è¯¦ç»†è§£æ")
    print("ğŸ”„ è¾“å…¥ 'info' æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯")
    print("ğŸ“Š è¾“å…¥ 'summary' æŸ¥çœ‹æ¨ç†è¿‡ç¨‹æ€»ç»“")
    print("âŒ è¾“å…¥ 'exit' æˆ–æŒ‰ Ctrl+C é€€å‡º")
    print("="*60)
    
    try:
        while True:
            dream = input("\nğŸ“ è¯·è¾“å…¥æ‚¨çš„æ¢¦å¢ƒæè¿° > ").strip()
            
            if not dream:
                continue
                
            if dream.lower() == 'exit':
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            elif dream.lower() == 'info':
                info = interpreter.get_model_info()
                print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
                print(f"   æ¨¡å‹åç§°: {info['model_name']}")
                print(f"   è¿è¡Œè®¾å¤‡: {info['device']}")
                print(f"   åŠ è½½æ­¥éª¤: {info['total_loading_steps']} æ­¥")
                continue
            elif dream.lower() == 'summary':
                interpreter.print_inference_summary()
                continue
            
            print(f"\nğŸ”® æ­£åœ¨è§£ææ‚¨çš„æ¢¦å¢ƒ...")
            print("-" * 40)
            
            try:
                result = interpreter.interpret(dream)
                
                print("\n" + "="*60)
                print("ğŸŒŸ æ¢¦å¢ƒè§£æç»“æœ:")
                print("="*60)
                print(result)
                print("="*60)
                
                # æ˜¾ç¤ºæ¨ç†æ€»ç»“
                interpreter.print_inference_summary()
                
            except Exception as e:
                print(f"âŒ è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥è¾“å…¥æˆ–ç¨åé‡è¯•")
                
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ¢¦å¢ƒè§£æAIåŠ©æ‰‹ï¼Œå†è§ï¼")
