#!/usr/bin/env python3
"""
æœ¬åœ°æ¨¡å‹åŠ è½½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æœ¬åœ°DeepSeek 7Bæ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸åŠ è½½
"""

import os
import torch
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer

def test_local_model_loading():
    """æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½"""
    print("ğŸ” å¼€å§‹æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½...")
    
    # å¸¸è§çš„æœ¬åœ°æ¨¡å‹å­˜å‚¨è·¯å¾„
    possible_paths = [
        # Windowså¸¸è§è·¯å¾„
        "C:\\Users\\%USERNAME%\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B",
        "D:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
        "E:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
        os.path.expanduser("~\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B"),
        # ç›¸å¯¹è·¯å¾„
        "./models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "../models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    ]
    
    # æ‰©å±•ç¯å¢ƒå˜é‡
    for i, path in enumerate(possible_paths):
        possible_paths[i] = os.path.expandvars(path)
    
    print("ğŸ“ æ£€æŸ¥å¯èƒ½çš„æ¨¡å‹è·¯å¾„:")
    found_path = None
    for path in possible_paths:
        print(f"  æ£€æŸ¥: {path}")
        if os.path.exists(path):
            print(f"  âœ… æ‰¾åˆ°æ¨¡å‹ç›®å½•: {path}")
            found_path = path
            break
        else:
            print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨")
    
    if not found_path:
        print("\nâŒ æœªæ‰¾åˆ°æœ¬åœ°DeepSeek 7Bæ¨¡å‹æ–‡ä»¶")
        print("\nğŸ’¡ å»ºè®®:")
        print("1. è¯·ç¡®è®¤æ¨¡å‹æ–‡ä»¶ä¸‹è½½ä½ç½®")
        print("2. è®¾ç½® MODEL_PATH ç¯å¢ƒå˜é‡æŒ‡å‘æ¨¡å‹ç›®å½•")
        print("3. æˆ–è€…å°†æ¨¡å‹æ–‡ä»¶æ”¾ç½®åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:")
        for path in possible_paths:
            print(f"   - {path}")
        return False
    
    print(f"\nğŸ§ª å°è¯•åŠ è½½æ¨¡å‹: {found_path}")
    
    try:
        # æµ‹è¯•åˆ†è¯å™¨åŠ è½½
        print("ğŸ“š åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            found_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("ğŸ§  åŠ è½½æ¨¡å‹...")
        load_kwargs = {
            "trust_remote_code": True,
            "local_files_only": True,
        }
        
        if torch.cuda.is_available():
            load_kwargs["torch_dtype"] = torch.float16
            load_kwargs["device_map"] = "auto"
            print(f"ğŸ® ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        else:
            load_kwargs["torch_dtype"] = torch.float32
            print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
        
        model = AutoModelForCausalLM.from_pretrained(
            found_path,
            **load_kwargs
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨ç†
        print("\nğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½...")
        test_text = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
        messages = [{"role": "user", "content": test_text}]
        
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        
        inputs = tokenizer(text_input, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸï¼Œå“åº”: {response[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_local_model_loading()
    sys.exit(0 if success else 1)