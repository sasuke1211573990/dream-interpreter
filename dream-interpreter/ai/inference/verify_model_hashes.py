#!/usr/bin/env python3
"""
æ¨¡å‹æ–‡ä»¶å“ˆå¸Œæ ¡éªŒå·¥å…·
"""
import os
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Optional

def calculate_file_hash(file_path: str, algorithm: str = "sha256") -> str:
    """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    hash_obj = hashlib.new(algorithm)
    
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_obj.update(chunk)
        return hash_obj.hexdigest()
    except Exception as e:
        print(f"è®¡ç®—å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        return ""

def find_model_files(model_path: str) -> List[str]:
    """æŸ¥æ‰¾æ¨¡å‹ç›¸å…³æ–‡ä»¶"""
    model_files = []
    
    if not os.path.exists(model_path):
        return model_files
    
    # å¸¸è§æ¨¡å‹æ–‡ä»¶æ‰©å±•å
    model_extensions = [".bin", ".safetensors", ".pt", ".pth", ".ckpt"]
    
    # æ£€æŸ¥ç›®å½•ä¸­çš„æ–‡ä»¶
    for root, dirs, files in os.walk(model_path):
        for file in files:
            file_path = os.path.join(root, file)
            
            # åŒ…å«æ¨¡å‹æ–‡ä»¶æ‰©å±•åæˆ–è€…æ˜¯é‡è¦é…ç½®æ–‡ä»¶
            if any(file.endswith(ext) for ext in model_extensions) or \
               file in ["config.json", "tokenizer.json", "tokenizer_config.json", "pytorch_model.bin.index.json", "model.safetensors.index.json"]:
                model_files.append(file_path)
    
    return model_files

def verify_model_hashes(model_path: str, reference_hashes: Optional[Dict[str, str]] = None) -> Dict:
    """éªŒè¯æ¨¡å‹æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    print(f"ğŸ” æ­£åœ¨æ‰«ææ¨¡å‹ç›®å½•: {model_path}")
    
    model_files = find_model_files(model_path)
    
    if not model_files:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return {"status": "error", "message": "æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶"}
    
    print(f"ğŸ“ æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹ç›¸å…³æ–‡ä»¶")
    
    results = {
        "model_path": model_path,
        "total_files": len(model_files),
        "hashes": {},
        "missing_files": [],
        "mismatched_files": [],
        "verified_files": []
    }
    
    # è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„å“ˆå¸Œå€¼
    for file_path in model_files:
        file_name = os.path.basename(file_path)
        print(f"ğŸ”„ æ­£åœ¨è®¡ç®— {file_name} çš„å“ˆå¸Œå€¼...")
        
        file_hash = calculate_file_hash(file_path)
        if file_hash:
            results["hashes"][file_name] = file_hash
            
            # å¦‚æœæœ‰å‚è€ƒå“ˆå¸Œå€¼ï¼Œè¿›è¡Œæ¯”å¯¹
            if reference_hashes and file_name in reference_hashes:
                if file_hash == reference_hashes[file_name]:
                    results["verified_files"].append(file_name)
                    print(f"âœ… {file_name}: å“ˆå¸Œå€¼åŒ¹é…")
                else:
                    results["mismatched_files"].append(file_name)
                    print(f"âŒ {file_name}: å“ˆå¸Œå€¼ä¸åŒ¹é…")
            else:
                print(f"â„¹ï¸  {file_name}: {file_hash[:16]}...")
    
    # æ£€æŸ¥ç¼ºå¤±çš„æ–‡ä»¶
    if reference_hashes:
        for ref_file in reference_hashes.keys():
            if ref_file not in [os.path.basename(f) for f in model_files]:
                results["missing_files"].append(ref_file)
                print(f"âš ï¸  ç¼ºå¤±æ–‡ä»¶: {ref_file}")
    
    # æ€»ä½“çŠ¶æ€
    if reference_hashes:
        if not results["mismatched_files"] and not results["missing_files"]:
            results["status"] = "verified"
            print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å“ˆå¸Œå€¼éªŒè¯é€šè¿‡!")
        else:
            results["status"] = "mismatch"
            print(f"\nâš ï¸  å‘ç° {len(results['mismatched_files'])} ä¸ªæ–‡ä»¶å“ˆå¸Œå€¼ä¸åŒ¹é…, {len(results['missing_files'])} ä¸ªæ–‡ä»¶ç¼ºå¤±")
    else:
        results["status"] = "calculated"
        print(f"\nâœ… å·²å®Œæˆ {len(results['hashes'])} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼è®¡ç®—")
    
    return results

def save_hashes_to_file(hashes: Dict[str, str], output_file: str):
    """ä¿å­˜å“ˆå¸Œå€¼åˆ°æ–‡ä»¶"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(hashes, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å“ˆå¸Œå€¼å·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å“ˆå¸Œå€¼å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” DeepSeekæ¨¡å‹æ–‡ä»¶å“ˆå¸Œæ ¡éªŒå·¥å…·")
    print("=" * 50)
    
    # è‡ªåŠ¨æ£€æµ‹æœ¬åœ°æ¨¡å‹è·¯å¾„
    possible_paths = [
        os.path.expanduser("~\.cache\huggingface\hub\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B\snapshots"),
        "D:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
        "E:\\models\\deepseek-ai\\DeepSeek-R1-Distill-Qwen-7B",
        "./models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "../models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    ]
    
    model_path = None
    for path in possible_paths:
        if os.path.exists(path):
            # å¦‚æœæ˜¯snapshotsç›®å½•ï¼ŒæŸ¥æ‰¾æœ€æ–°ç‰ˆæœ¬
            if "snapshots" in path:
                try:
                    subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]
                    if subdirs:
                        latest_snapshot = os.path.join(path, sorted(subdirs)[-1])
                        config_path = os.path.join(latest_snapshot, "config.json")
                        if os.path.exists(config_path):
                            model_path = latest_snapshot
                            break
                except Exception:
                    pass
            else:
                config_path = os.path.join(path, "config.json")
                if os.path.exists(config_path):
                    model_path = path
                    break
    
    if not model_path:
        print("âŒ æœªæ‰¾åˆ°æœ¬åœ°DeepSeekæ¨¡å‹")
        print("è¯·æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä¾‹å¦‚:")
        print("python verify_model_hashes.py --path /path/to/model")
        return
    
    print(f"ğŸ“ æ£€æµ‹åˆ°æ¨¡å‹è·¯å¾„: {model_path}")
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦ä¿å­˜å“ˆå¸Œå€¼
    save_hashes = input("æ˜¯å¦ä¿å­˜è®¡ç®—å‡ºçš„å“ˆå¸Œå€¼åˆ°æ–‡ä»¶? (y/n): ").lower().strip() == 'y'
    
    # æ‰§è¡Œå“ˆå¸Œæ ¡éªŒ
    results = verify_model_hashes(model_path)
    
    # ä¿å­˜ç»“æœ
    if save_hashes and results["hashes"]:
        output_file = "deepseek_model_hashes.json"
        save_hashes_to_file(results["hashes"], output_file)
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æ ¡éªŒç»“æœæ€»ç»“:")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æ€»æ–‡ä»¶æ•°: {results['total_files']}")
    print(f"è®¡ç®—å“ˆå¸Œ: {len(results['hashes'])}")
    
    if results["status"] == "verified":
        print("âœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡")
    elif results["status"] == "mismatch":
        print(f"âš ï¸  {len(results['mismatched_files'])} ä¸ªæ–‡ä»¶ä¸åŒ¹é…")
        if results["mismatched_files"]:
            print("ä¸åŒ¹é…æ–‡ä»¶:")
            for f in results["mismatched_files"]:
                print(f"  - {f}")
    elif results["status"] == "calculated":
        print("âœ… å“ˆå¸Œå€¼è®¡ç®—å®Œæˆ")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DeepSeekæ¨¡å‹æ–‡ä»¶å“ˆå¸Œæ ¡éªŒå·¥å…·")
    parser.add_argument("--path", type=str, help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--reference", type=str, help="å‚è€ƒå“ˆå¸Œå€¼JSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, help="è¾“å‡ºå“ˆå¸Œå€¼åˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    if args.path:
        # ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
        reference_hashes = None
        if args.reference:
            try:
                with open(args.reference, 'r') as f:
                    reference_hashes = json.load(f)
            except Exception as e:
                print(f"âŒ è¯»å–å‚è€ƒå“ˆå¸Œæ–‡ä»¶å¤±è´¥: {e}")
                sys.exit(1)
        
        results = verify_model_hashes(args.path, reference_hashes)
        
        if args.output:
            save_hashes_to_file(results["hashes"], args.output)
    else:
        # äº¤äº’æ¨¡å¼
        main()